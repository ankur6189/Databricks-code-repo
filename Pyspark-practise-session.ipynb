{"cells":[{"cell_type":"markdown","source":["## Overview\n\nThis notebook will show you how to create and query a table or DataFrame that you uploaded to DBFS. [DBFS](https://docs.databricks.com/user-guide/dbfs-databricks-file-system.html) is a Databricks File System that allows you to store data for querying inside of Databricks. This notebook assumes that you have a file already inside of DBFS that you would like to read from.\n\nThis notebook is written in **Python** so the default cell type is Python. However, you can use different languages by using the `%LANGUAGE` syntax. Python, Scala, SQL, and R are all supported."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"96816ed7-b08a-4ca3-abb9-f99880c3535d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# File location and type\nfile_location = \"/FileStore/tables/ds_salaries.csv\"\nfile_path = \"/FileStore/shared_uploads/a.shanker.srivastava@accenture.com/sample_data.txt\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"true\"\nfirst_row_is_header = \"true\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndf1=spark.read.text(file_path)\n\ndisplay(df1)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6482be4c-f067-47c9-b0ac-35c938b94601","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["A Precis is a summary of a story. The gist of the passage is written in as few words as possible. The reader should understand the idea expressed in the first reading by only reading the Precis. So it should have all the essential points. It is an exercise of comprehension. It must always be shorter than the original passage. Many people read carelessly and fail to understand the meaning of the passage for Precis Writing. So, it helps them to pay attention to the reading as no one writes a Precis without reading it carefully."],["Summarizing teaches one to read with concentration. It improves the overall Writing skills too. It allows one to express their thoughts clearly, concisely, and effectively. You learn to choose your words carefully and in a logical manner."],["In this article, students will be learning about what is Precis Writing, the Importance of Precis Writing, Writing Precis, Steps for Precis Writing, Features of a Good Precis, Dos and Don’ts for Precis Writing, Characteristics of Precis Elements of Precis along with respective examples."],["Students can go through the official website of Vedantu to get a thorough understanding of various topics related to the English language."],["The Precis Writing format has some essential points that you should take care of while writing it. The main important point for Writing a Precis is that you should read the passage carefully. Reading helps you to understand the gist of the passage and write it down more concisely. The main message and theme of the passage should be conveyed through your Precis."],["Also, it is essential to note down that the general idea of the passage for Precis Writing should not be forgotten. The key points should be written in the Precis. The language of the Precis should be focused on. Precis Writing has no fixed pattern. However, we need to follow a particular procedure for the Precis to be more effective. The right diction will help you write a good Precis."]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"value","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .table-result-container {\n","    max-height: 300px;\n","    overflow: auto;\n","  }\n","  table, th, td {\n","    border: 1px solid black;\n","    border-collapse: collapse;\n","  }\n","  th, td {\n","    padding: 5px;\n","  }\n","  th {\n","    text-align: left;\n","  }\n","</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>value</th></tr></thead><tbody><tr><td>A Precis is a summary of a story. The gist of the passage is written in as few words as possible. The reader should understand the idea expressed in the first reading by only reading the Precis. So it should have all the essential points. It is an exercise of comprehension. It must always be shorter than the original passage. Many people read carelessly and fail to understand the meaning of the passage for Precis Writing. So, it helps them to pay attention to the reading as no one writes a Precis without reading it carefully.</td></tr><tr><td>Summarizing teaches one to read with concentration. It improves the overall Writing skills too. It allows one to express their thoughts clearly, concisely, and effectively. You learn to choose your words carefully and in a logical manner.</td></tr><tr><td>In this article, students will be learning about what is Precis Writing, the Importance of Precis Writing, Writing Precis, Steps for Precis Writing, Features of a Good Precis, Dos and Don’ts for Precis Writing, Characteristics of Precis Elements of Precis along with respective examples.</td></tr><tr><td>Students can go through the official website of Vedantu to get a thorough understanding of various topics related to the English language.</td></tr><tr><td>The Precis Writing format has some essential points that you should take care of while writing it. The main important point for Writing a Precis is that you should read the passage carefully. Reading helps you to understand the gist of the passage and write it down more concisely. The main message and theme of the passage should be conveyed through your Precis.</td></tr><tr><td>Also, it is essential to note down that the general idea of the passage for Precis Writing should not be forgotten. The key points should be written in the Precis. The language of the Precis should be focused on. Precis Writing has no fixed pattern. However, we need to follow a particular procedure for the Precis to be more effective. The right diction will help you write a good Precis.</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import split,explode,lower,upper,max,count\nres=df1.withColumn(\"splt\",explode(split(df1.value,\" \")))\nres_df=res.withColumn(\"low\",lower(res.splt)).drop(res.splt)\nres_df=res_df.groupBy(res_df.low).count().sort('count', ascending=False)\nres_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0dadd5a1-7e19-40a5-9279-5c50fd317c63","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+-----+\n|      low|count|\n+---------+-----+\n|      the|   30|\n|   precis|   15|\n|       of|   15|\n|       to|   12|\n|        a|   10|\n|       it|    9|\n|   should|    8|\n|  writing|    7|\n|       be|    7|\n|      for|    6|\n|       is|    6|\n|  passage|    6|\n|      and|    6|\n|  reading|    5|\n|      you|    5|\n|       in|    5|\n|  precis.|    4|\n| writing,|    4|\n|essential|    3|\n|      one|    3|\n+---------+-----+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import col, count,desc,collect_set,explode,lit,split,element_at,when\nfrom pyspark.sql.types import *\ndf.withColumn(\"expt\", when(df.salary<80000,df.salary).otherwise(\"Ameer\")).show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"bd82bb99-1479-4d5c-be10-8c36df0f1d44","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+-----+\n|work_year|experience_level|employment_type|           job_title|salary|salary_currency|salary_in_usd|employee_residence|remote_ratio|company_location|company_size| expt|\n+---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+-----+\n|     2023|              SE|             FT|Principal Data Sc...| 80000|            EUR|        85847|                ES|         100|              ES|           L|Ameer|\n|     2023|              MI|             CT|         ML Engineer| 30000|            USD|        30000|                US|         100|              US|           S|30000|\n|     2023|              MI|             CT|         ML Engineer| 25500|            USD|        25500|                US|         100|              US|           S|25500|\n|     2023|              SE|             FT|      Data Scientist|175000|            USD|       175000|                CA|         100|              CA|           M|Ameer|\n|     2023|              SE|             FT|      Data Scientist|120000|            USD|       120000|                CA|         100|              CA|           M|Ameer|\n|     2023|              SE|             FT|   Applied Scientist|222200|            USD|       222200|                US|           0|              US|           L|Ameer|\n|     2023|              SE|             FT|   Applied Scientist|136000|            USD|       136000|                US|           0|              US|           L|Ameer|\n|     2023|              SE|             FT|      Data Scientist|219000|            USD|       219000|                CA|           0|              CA|           M|Ameer|\n|     2023|              SE|             FT|      Data Scientist|141000|            USD|       141000|                CA|           0|              CA|           M|Ameer|\n|     2023|              SE|             FT|      Data Scientist|147100|            USD|       147100|                US|           0|              US|           M|Ameer|\n+---------+----------------+---------------+--------------------+------+---------------+-------------+------------------+------------+----------------+------------+-----+\nonly showing top 10 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["res_df=df.groupBy(df.job_title).agg(count(df.job_title).alias(\"emp_count\")).orderBy(desc(\"emp_count\"))\nres_df.show(5)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b5f66379-6f7f-42ec-8e82-d0e0926a1721","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+---------+\n|           job_title|emp_count|\n+--------------------+---------+\n|       Data Engineer|     1040|\n|      Data Scientist|      840|\n|        Data Analyst|      612|\n|Machine Learning ...|      289|\n|  Analytics Engineer|      103|\n+--------------------+---------+\nonly showing top 5 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import collect_list,size\nres_df=df.groupBy(df.job_title).agg(collect_list(df.employment_type).alias(\"emp_list\"))\n#res_df.show()\n\n\n#res_df=res_df.withColumn(\"title_count\",res_df.groupby(res_df.job_title).agg(count(res_df.title)))\n#res_df.show()\n#df.filter(df.job_title==\"3D Computer Vision Researcher\").select(df.employment_type).distinct().show()\n#\n#res_df.withColumn(\"First Name\",split(df.job_title,\" \")[0]).withColumn(\"Last Name\",split(df.job_title,\" \")).show(10)\n#res_df.withColumn(\"exploded_df\",explode(res_df.emp_list)).show()\n\n#new_df=res_df.withColumn(\"Sentence\",lit(\"The random sentence Generator contains 1000+ random sentence created specifically for this free writing tool and found nowhere else.\")).limit(1)\n#new_df.withColumn(\"exploded_column\", explode(split(new_df.Sentence,\" \"))).groupBy('exploded_column').count().show()\n\n#new_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"e314f376-6166-4f3d-aae2-c7a64dcf7801","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+--------------------+-----------+\n|           job_title|            emp_list|title_count|\n+--------------------+--------------------+-----------+\n|3D Computer Visio...|    [FT, FT, FT, PT]|          4|\n|  Lead Data Engineer|[FT, FT, FT, FT, ...|          6|\n|        Data Modeler|            [FT, FT]|          2|\n| Data Scientist Lead|            [FT, FT]|          2|\n|Principal Data Ar...|                [FT]|          1|\n|Head of Machine L...|                [FT]|          1|\n|Machine Learning ...|[FT, FT, FT, FT, ...|         10|\n|Data Analytics Sp...|            [FT, FT]|          2|\n|     Data Specialist|[FT, FT, FT, FT, ...|         14|\n|Data Operations E...|[FT, FT, FT, FT, ...|         10|\n|Deep Learning Res...|                [FT]|          1|\n| Data Analytics Lead|            [FT, FT]|          2|\n|  Power BI Developer|                [FT]|          1|\n|Machine Learning ...|[FT, FT, FT, FT, ...|         26|\n|   Lead Data Analyst|[FT, FT, FT, FT, FT]|          5|\n|        BI Developer|[FT, FT, FT, FT, ...|         13|\n|Staff Data Scientist|                [CT]|          1|\n|       ETL Developer|[FT, FT, FT, FT, ...|         10|\n|           Data Lead|            [FT, FT]|          2|\n|Product Data Scie...|                [FT]|          1|\n+--------------------+--------------------+-----------+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["res_df=res_df.withColumn(\"title\",explode(res_df.emp_list))\nres_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"7a9147ac-21f7-4ee5-bf4c-ea9d3c2d5a41","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+--------------------+-----+\n|           job_title|            emp_list|title|\n+--------------------+--------------------+-----+\n|3D Computer Visio...|    [FT, FT, FT, PT]|   FT|\n|3D Computer Visio...|    [FT, FT, FT, PT]|   FT|\n|3D Computer Visio...|    [FT, FT, FT, PT]|   FT|\n|3D Computer Visio...|    [FT, FT, FT, PT]|   PT|\n|  Lead Data Engineer|[FT, FT, FT, FT, ...|   FT|\n|  Lead Data Engineer|[FT, FT, FT, FT, ...|   FT|\n|  Lead Data Engineer|[FT, FT, FT, FT, ...|   FT|\n|  Lead Data Engineer|[FT, FT, FT, FT, ...|   FT|\n|  Lead Data Engineer|[FT, FT, FT, FT, ...|   FT|\n|  Lead Data Engineer|[FT, FT, FT, FT, ...|   FT|\n|        Data Modeler|            [FT, FT]|   FT|\n|        Data Modeler|            [FT, FT]|   FT|\n| Data Scientist Lead|            [FT, FT]|   FT|\n| Data Scientist Lead|            [FT, FT]|   FT|\n|Principal Data Ar...|                [FT]|   FT|\n|Head of Machine L...|                [FT]|   FT|\n|Machine Learning ...|[FT, FT, FT, FT, ...|   FT|\n|Machine Learning ...|[FT, FT, FT, FT, ...|   FT|\n|Machine Learning ...|[FT, FT, FT, FT, ...|   FT|\n|Machine Learning ...|[FT, FT, FT, FT, ...|   FT|\n+--------------------+--------------------+-----+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["total=res_df.withColumn(\"title_count\",size(res_df.emp_list))\ntotal.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"6c943a1b-1320-4563-827d-030d9067d615","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+--------------------+-----+-----------+\n|           job_title|            emp_list|title|title_count|\n+--------------------+--------------------+-----+-----------+\n|3D Computer Visio...|    [FT, FT, FT, PT]|   FT|          4|\n|3D Computer Visio...|    [FT, FT, FT, PT]|   FT|          4|\n|3D Computer Visio...|    [FT, FT, FT, PT]|   FT|          4|\n|3D Computer Visio...|    [FT, FT, FT, PT]|   PT|          4|\n|  Lead Data Engineer|[FT, FT, FT, FT, ...|   FT|          6|\n|  Lead Data Engineer|[FT, FT, FT, FT, ...|   FT|          6|\n|  Lead Data Engineer|[FT, FT, FT, FT, ...|   FT|          6|\n|  Lead Data Engineer|[FT, FT, FT, FT, ...|   FT|          6|\n|  Lead Data Engineer|[FT, FT, FT, FT, ...|   FT|          6|\n|  Lead Data Engineer|[FT, FT, FT, FT, ...|   FT|          6|\n|        Data Modeler|            [FT, FT]|   FT|          2|\n|        Data Modeler|            [FT, FT]|   FT|          2|\n| Data Scientist Lead|            [FT, FT]|   FT|          2|\n| Data Scientist Lead|            [FT, FT]|   FT|          2|\n|Principal Data Ar...|                [FT]|   FT|          1|\n|Head of Machine L...|                [FT]|   FT|          1|\n|Machine Learning ...|[FT, FT, FT, FT, ...|   FT|         10|\n|Machine Learning ...|[FT, FT, FT, FT, ...|   FT|         10|\n|Machine Learning ...|[FT, FT, FT, FT, ...|   FT|         10|\n|Machine Learning ...|[FT, FT, FT, FT, ...|   FT|         10|\n+--------------------+--------------------+-----+-----------+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["# With this registered as a temp view, it will only be available to this particular notebook. If you'd like other users to be able to query this table, you can also create a table from the DataFrame.\n# Once saved, this table will persist across cluster restarts as well as allow various users across different notebooks to query this data.\n# To do so, choose your table name and uncomment the bottom line.\n\npermanent_table_name = \"ds_salaries_csv\"\n\n# df.write.format(\"parquet\").saveAsTable(permanent_table_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"db9631f6-bb4a-42ca-8a3c-0d48af932331","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"","metadata":{},"errorTraceType":null,"type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"2023-06-27 - Pyspark-practise-session","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
